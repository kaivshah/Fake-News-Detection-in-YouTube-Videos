{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dependencies"
      ],
      "metadata": {
        "id": "Lncis0GoSCep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "import itertools\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "YDZwtJbUSIYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing stopwords removing package\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm0BkXiQUXuY",
        "outputId": "aa20f94d-8355-4f9f-96e1-572fcbba752c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "QspRp5lVU0Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a20540-393a-4227-fad4-56d6472e2059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "_s0oxYKRV3T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading dataset to pandas dataframe\n",
        "news_dataset = pd.read_csv('/content/file.csv',encoding='latin1')\n",
        "#response = news_dataset.recv(1024).decode('utf-8', 'ignore')\n"
      ],
      "metadata": {
        "id": "JMLdVukfV6b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "5390eb56-f589-4366-ea97-cc3939dd6c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f2e9598c60ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading dataset to pandas dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnews_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/file.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#response = news_dataset.recv(1024).decode('utf-8', 'ignore')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/file.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rows and Columns in dataset\n",
        "news_dataset.shape"
      ],
      "metadata": {
        "id": "oN2RLYSsWHyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the dataset \n",
        "news_dataset.head()\n"
      ],
      "metadata": {
        "id": "48Fys_P4bTlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the dataset\n",
        "labels=news_dataset.normalized_annotation\n",
        "print(labels.head())\n",
        "\n",
        "target =news_dataset.normalized_annotation.value_counts()\n",
        "print(target)\n",
        "\n",
        "sns.countplot(news_dataset.normalized_annotation)\n",
        "plt.title(\"Distribution of Real and Fake News\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r_KnXZQ-fwlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#counting number of missing values\n",
        "news_dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "uZF87wLZbbVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cr3D0h0P7rmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relacing null values\n",
        "news_dataset = news_dataset.fillna('')"
      ],
      "metadata": {
        "id": "Jd--qeGebqIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging author name and news title\n",
        "#news_dataset['content'] = news_dataset['author']+news_dataset['title']"
      ],
      "metadata": {
        "id": "Z6aBmxg5cGbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(news_dataset['content'])"
      ],
      "metadata": {
        "id": "6-ErXTNCcfiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separating the data & label\n",
        "#X = news_dataset.drop(columns='label',axis=1)\n",
        "#X = news_dataset.drop(news_dataset.columns[[0,1,2,3,4,5,6,7,8,9]],axis=1)\n",
        "X = news_dataset['Captions']\n",
        "#Y = news_dataset['label']\n",
        "Y = news_dataset['normalized_annotation']"
      ],
      "metadata": {
        "id": "4kI1mXWJckX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#separating the data and label\n",
        "X = news_dataset['Captions'].values\n",
        "Y = news_dataset['normalized_annotation'].values"
      ],
      "metadata": {
        "id": "LjFRrmIjynQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "DmvWQeqOdFVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n"
      ],
      "metadata": {
        "id": "fmmFiQ8ldh9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "port_stem = PorterStemmer()\n"
      ],
      "metadata": {
        "id": "u7XP8JZqdjhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(content):\n",
        "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
        "    stemmed_content = stemmed_content.lower()\n",
        "    stemmed_content = stemmed_content.split()\n",
        "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
        "    stemmed_content = ' '.join(stemmed_content)\n",
        "    return stemmed_content"
      ],
      "metadata": {
        "id": "AIBysj5ehsLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_dataset['Captions'] = news_dataset['Captions'].apply(stemming)\n"
      ],
      "metadata": {
        "id": "FRnMRWjVibrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(news_dataset['Captions'])"
      ],
      "metadata": {
        "id": "qGGK1n38ieaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#separating the data and label\n",
        "X = news_dataset['Captions'].values\n",
        "Y = news_dataset['normalized_annotation'].values"
      ],
      "metadata": {
        "id": "h9A60JW_i24G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "wNxCSPf9i6mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y)"
      ],
      "metadata": {
        "id": "e0HkhPDBi9eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "id": "oDakQEUai_hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the textual data to numerical data\n",
        "vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7)\n",
        "vectorizer.fit(X)\n",
        "\n",
        "X = vectorizer.transform(X)"
      ],
      "metadata": {
        "id": "TR8n-XZyjDv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "l6iFQxYml1p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spilting the dataset to training & test data"
      ],
      "metadata": {
        "id": "_GGdgmIYmN1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3, random_state=85)\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)"
      ],
      "metadata": {
        "id": "x9smIhfpmWkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_ha0WqpSD3NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model : Logistic Regression"
      ],
      "metadata": {
        "id": "CbuFynQ4nerG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()"
      ],
      "metadata": {
        "id": "xxgK8zLBnkfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "YDXov4jYnoal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "S2oFoOEDnwrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy score"
      ],
      "metadata": {
        "id": "STvXm_r7n8nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy score on the training data\n",
        "X_train_prediction = model.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)"
      ],
      "metadata": {
        "id": "agoLn6Fmn_0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy score of the training data : ', training_data_accuracy)"
      ],
      "metadata": {
        "id": "7Btro8lNoIdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy score on the test data\n",
        "X_test_prediction = model.predict(X_test)\n",
        "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)"
      ],
      "metadata": {
        "id": "7Xdvi5MvoK-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy score of the test data : ', test_data_accuracy)"
      ],
      "metadata": {
        "id": "3ZjL7q-3ocOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cf_matrix=confusion_matrix(Y_train,X_train_prediction)\n",
        "print(cf_matrix)"
      ],
      "metadata": {
        "id": "jFIKw2G-O4W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Purples):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "o5a1v6F0X5-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# model\n",
        "def compute_metrics(data, y_true, model_obj, model):\n",
        "\n",
        "  # Make predictions\n",
        "  y_pred = model_obj.predict(data)\n",
        "\n",
        "  # Compute accuracy\n",
        "  acc = accuracy_score(y_true = y_true, y_pred = y_pred)\n",
        "\n",
        "  # Make DataFrame\n",
        "  metrics = pd.DataFrame(data = np.array([acc]), index=[model], columns=['Accuracy Score'])\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "R7Hnh3Av-tdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "prediction1 = model.predict(X_test)\n",
        "score = metrics.accuracy_score(Y_test, prediction1)\n",
        "print(\"Testing accuracy:   %0.3f\" % score)\n",
        "cm1 = metrics.confusion_matrix(Y_test,prediction1)\n",
        "plot_confusion_matrix(cm1, classes=['FAKE', 'REAL','DEBUNKING'])"
      ],
      "metadata": {
        "id": "TRP_TrVz0p26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "H8tiP4ob_ISP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "WVcjFYFR-PEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "y6HbC1moGJpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's implement the model : Multinomial Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier=MultinomialNB()"
      ],
      "metadata": {
        "id": "4uMjvYfv-SET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "classifier.fit(X1_train, Y1_train)\n",
        "\n",
        "\n",
        "prediction1 = classifier.predict(X1_test)\n",
        "score = metrics.accuracy_score(Y1_test, prediction1)\n",
        "print(\"Testing Accuracy:   %0.3f\" % score)\n",
        "cm1 = metrics.confusion_matrix(Y_train,X_train_prediction)\n",
        "plot_confusion_matrix(cm1, classes=['FAKE', 'REAL','DEBUNKING'])"
      ],
      "metadata": {
        "id": "1UQEn9Mn-R7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passive Aggressive"
      ],
      "metadata": {
        "id": "GIb7ZXD6VlN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "pmNxp_TfGrxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = PassiveAggressiveClassifier()\n",
        "classifier.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "P7tE2h9YW_qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets implemet the algorithm : Passive Aggressive Classifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "linear_clf = PassiveAggressiveClassifier(max_iter=50)\n",
        "\n",
        "linear_clf.fit(X2_train, Y2_train)\n",
        "\n",
        "prediction1 = linear_clf.predict(X2_train)\n",
        "train_score = metrics.accuracy_score(Y2_train, prediction1)\n",
        "print(\"Training Accuracy:   %0.3f\" % train_score)\n",
        "\n",
        "prediction2 = linear_clf.predict(X2_test)\n",
        "score = metrics.accuracy_score(Y2_test, prediction2)\n",
        "print(\"Testing Accuracy:   %0.3f\" % score)\n",
        "cm2 = metrics.confusion_matrix(Y_train,X_train_prediction)\n",
        "plot_confusion_matrix(cm2, classes=['FAKE Data', 'REAL Data','DEBUNKING'])"
      ],
      "metadata": {
        "id": "GcqJ_qXEGvjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = classifier.predict(X_test)\n",
        "score = metrics.accuracy_score(Y_test, Y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')\n",
        "cm = metrics.confusion_matrix(Y_test, Y_pred)\n",
        "#plot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data'])"
      ],
      "metadata": {
        "id": "D1W7H8UhXWzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "lR1MynHJ-Nxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "Os2yEnCh-M0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RFC= RandomForestClassifier(random_state=0)\n",
        "RFC.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "w6tjVXI_-lFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RFC.score(X_test,Y_test)"
      ],
      "metadata": {
        "id": "srycXdkn-k27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "RFC.fit(X_train, Y_train)\n",
        "\n",
        "prediction1 = RFC.predict(X_test)\n",
        "score = metrics.accuracy_score(Y_test, prediction1)\n",
        "print(\"Testing Accuracy:   %0.3f\" % score)\n",
        "cm1 = metrics.confusion_matrix(Y_test,prediction1)\n",
        "plot_confusion_matrix(cm1, classes=['FAKE', 'REAL','DEBUNKING'])"
      ],
      "metadata": {
        "id": "5_XygO6z_lzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WJgGW6LW-kpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree"
      ],
      "metadata": {
        "id": "mXyyMP01Dbvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "lKLZIZzvQvmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DT=DecisionTreeClassifier()\n",
        "#DT.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "UZl4ZLnDRlhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy score on the training data\n",
        "#X_train_prediction = DT.predict(X_train)\n",
        "#training_data_accuracy = accuracy_score(X_train_prediction, Y_train)"
      ],
      "metadata": {
        "id": "a_Grvk2ySwOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print('Accuracy score of the training data : ', training_data_accuracy)"
      ],
      "metadata": {
        "id": "qfWEhVRqTbbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy score on the test data\n",
        "#X_test_prediction = DT.predict(X_test)\n",
        "#test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n",
        "#print('Accuracy score of the test data : ', test_data_accuracy)"
      ],
      "metadata": {
        "id": "DxTk6-mlTtc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "import pickle\n",
        "filename=\"finalized_model.pkl\"\n",
        "pickle.dump(linear_clf,open(filename,'wb'))"
      ],
      "metadata": {
        "id": "gOIxro3zMlw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save vectorizer\n",
        "filename=\"vectorizer.pkl\"\n",
        "pickle.dump(vectorizer,open(filename,'wb'))"
      ],
      "metadata": {
        "id": "EhbItm_HNGwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making predictive system"
      ],
      "metadata": {
        "id": "XqG7ICetqEPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = X_test[5]\n",
        "\n",
        "prediction = model.predict(X_new)\n",
        "print(prediction)\n",
        "\n",
        "if (prediction[0]==0):\n",
        "  print('The news is Real')\n",
        "elif(prediction[0]==-1):\n",
        "  print('It is a debunking news')\n",
        "else:\n",
        "  print('The news is fake')"
      ],
      "metadata": {
        "id": "AVOfaMiyqJNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_test[28])"
      ],
      "metadata": {
        "id": "kU6tGg_4qWef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kVcHVbSGzoDc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}